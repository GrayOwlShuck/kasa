{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt_eng_twi_data_gen.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSw-klrLHXoT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "355697bb-f9b6-4304-f392-f99dabeca119"
      },
      "source": [
        "pip install tensorflow-gpu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/93/c7bca39b23aae45cd2e85ad3871c81eccc63b9c5276e926511e2e5b0879d/tensorflow_gpu-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 38kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.2)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 51.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.28.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 50.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.34.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-gpu) (46.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.21.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.2.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.8)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=ba831fe384d4a1abfec796d42e3ba044bbf865466a0c3ff3ae747485953d6b6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc3 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc3 has requirement tensorboard<2.3.0,>=2.2.0, but you'll have tensorboard 2.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc3 has requirement tensorflow-estimator<2.3.0,>=2.2.0rc0, but you'll have tensorflow-estimator 2.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, gast, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "Successfully installed gast-0.2.2 tensorboard-2.1.1 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_611pl3VHm5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcW3dxq1HvvO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "93307a83-949d-4b19-ba32-2b325d6963cc"
      },
      "source": [
        "# Path to the data txt file on\n",
        "data_path = \"/content/drive/My Drive/Dataset/Machine Translation/Others/en-tw.txt\"\n",
        "# open the file eng-spa.txt and read\n",
        "lines= pd.read_table(data_path,  names =['source', 'target'])\n",
        "#printing sample data from lines\n",
        "lines.sample(6)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16914</th>\n",
              "      <td>When you feel upset about something you have s...</td>\n",
              "      <td>Sɛ biribi a woaka anaa woayɛ haw wo a, akyinny...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16948</th>\n",
              "      <td>Except for the shallow breathing of the sleepi...</td>\n",
              "      <td>Esum ne nwura kata babiara ma ade a wote ara n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11173</th>\n",
              "      <td>(Exodus 14:4-31; 2 Kings 18:13–19:37) And thro...</td>\n",
              "      <td>(Exodus 14:4-31; 2 Ahene 18:13–19:37) Na Yehow...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2973</th>\n",
              "      <td>While waiting for the bus, they spend about te...</td>\n",
              "      <td>Bere a wɔretwɛn bɔs no, wɔde bɛyɛ simma du ken...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13434</th>\n",
              "      <td>Verse 34 states: “The alien resident who resid...</td>\n",
              "      <td>Leviticus 19:34 ka sɛ: “Ɔhɔho a wabɛtra mo mu ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10746</th>\n",
              "      <td>□ What is one of the most beautiful ways we ca...</td>\n",
              "      <td>□ Akwan pa a ɛsen biara a yebetumi afa so ayi ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  source                                             target\n",
              "16914  When you feel upset about something you have s...  Sɛ biribi a woaka anaa woayɛ haw wo a, akyinny...\n",
              "16948  Except for the shallow breathing of the sleepi...  Esum ne nwura kata babiara ma ade a wote ara n...\n",
              "11173  (Exodus 14:4-31; 2 Kings 18:13–19:37) And thro...  (Exodus 14:4-31; 2 Ahene 18:13–19:37) Na Yehow...\n",
              "2973   While waiting for the bus, they spend about te...  Bere a wɔretwɛn bɔs no, wɔde bɛyɛ simma du ken...\n",
              "13434  Verse 34 states: “The alien resident who resid...  Leviticus 19:34 ka sɛ: “Ɔhɔho a wabɛtra mo mu ...\n",
              "10746  □ What is one of the most beautiful ways we ca...  □ Akwan pa a ɛsen biara a yebetumi afa so ayi ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIAFEgyQIs5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert source and target text to Lowercase \n",
        "lines.source=lines.source.apply(lambda x: x.lower())\n",
        "lines.target=lines.target.apply(lambda x: x.lower())\n",
        "\n",
        "# Remove quotes from source and target text\n",
        "lines.source=lines.source.apply(lambda x: re.sub(\"'\", '', x))\n",
        "lines.target=lines.target.apply(lambda x: re.sub(\"'\", '', x))\n",
        "\n",
        "# create a set of all special characters\n",
        "special_characters= set(string.punctuation)\n",
        "\n",
        "# Remove all the special characters\n",
        "lines.source = lines.source.apply(lambda x: ''.join(char1 for char1 in x if char1 not in special_characters))\n",
        "lines.target = lines.target.apply(lambda x: ''.join(char1 for char1 in x if char1 not in special_characters))\n",
        "\n",
        "# Remove digits from source and target sentences\n",
        "num_digits= str.maketrans('','', digits)\n",
        "lines.source=lines.source.apply(lambda x: x.translate(num_digits))\n",
        "lines.target= lines.target.apply(lambda x: x.translate(num_digits))\n",
        "\n",
        "# Remove extra spaces\n",
        "lines.source=lines.source.apply(lambda x: x.strip())\n",
        "lines.target=lines.target.apply(lambda x: x.strip())\n",
        "lines.source=lines.source.apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "lines.target=lines.target.apply(lambda x: re.sub(\" +\", \" \", x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fjg021VI3pO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "26d5d44d-e3e5-48bf-b60a-4e7445dca299"
      },
      "source": [
        "# Add start and end tokens to target sequences\n",
        "lines.target = lines.target.apply(lambda x : 'START_ '+ x + ' _END')\n",
        "lines.sample(6)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7804</th>\n",
              "      <td>king david of israel was well aware of jehovah...</td>\n",
              "      <td>START_ ná israel hene dawid nim yehowa ahobamm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11260</th>\n",
              "      <td>in august susan and i accepted the invitation ...</td>\n",
              "      <td>START_ august no wɔtoo nsa frɛɛ me ne susan sɛ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19623</th>\n",
              "      <td>thank you english</td>\n",
              "      <td>START_ thank you borɔfo kasa _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1791</th>\n",
              "      <td>for instance some tend to get annoyed by every...</td>\n",
              "      <td>START_ sɛ nhwɛso no asetra mu ade ketewa biara...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8786</th>\n",
              "      <td>just as a car was about to hit her a sister gr...</td>\n",
              "      <td>START_ bere a kar bi rebɛbɔ no ara pɛ na onuaw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14936</th>\n",
              "      <td>we must never feel sorry for ourselves</td>\n",
              "      <td>START_ ɛnsɛ sɛ yenu yɛn ho da _END</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  source                                             target\n",
              "7804   king david of israel was well aware of jehovah...  START_ ná israel hene dawid nim yehowa ahobamm...\n",
              "11260  in august susan and i accepted the invitation ...  START_ august no wɔtoo nsa frɛɛ me ne susan sɛ...\n",
              "19623                                  thank you english                  START_ thank you borɔfo kasa _END\n",
              "1791   for instance some tend to get annoyed by every...  START_ sɛ nhwɛso no asetra mu ade ketewa biara...\n",
              "8786   just as a car was about to hit her a sister gr...  START_ bere a kar bi rebɛbɔ no ara pɛ na onuaw...\n",
              "14936             we must never feel sorry for ourselves                 START_ ɛnsɛ sɛ yenu yɛn ho da _END"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_Bs3qe2I95v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find all the source and target words and sort them\n",
        "# Vocabulary of Source language\n",
        "all_source_words=set()\n",
        "for source in lines.source:\n",
        "    for word in source.split():\n",
        "        if word not in all_source_words:\n",
        "            all_source_words.add(word)\n",
        "\n",
        "# Vocabulary of Target \n",
        "all_target_words=set()\n",
        "for target in lines.target:\n",
        "    for word in target.split():\n",
        "        if word not in all_target_words:\n",
        "            all_target_words.add(word)\n",
        "            \n",
        "# sort all unique source and target words\n",
        "source_words= sorted(list(all_source_words))\n",
        "target_words=sorted(list(all_target_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdpGXqSHJBAz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9a665a15-a860-4702-9497-05310f46cfb6"
      },
      "source": [
        "#Find maximum sentence length in  the source and target data\n",
        "source_length_list=[]\n",
        "for l in lines.source:\n",
        "    source_length_list.append(len(l.split(' ')))\n",
        "max_source_length= max(source_length_list)\n",
        "\n",
        "print(\" Max length of the source sentence\",max_source_length)\n",
        "target_length_list=[]\n",
        "\n",
        "for l in lines.target:\n",
        "    target_length_list.append(len(l.split(' ')))\n",
        "max_target_length= max(target_length_list)\n",
        "print(\" Max length of the target sentence\",max_target_length)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Max length of the source sentence 125\n",
            " Max length of the target sentence 112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA-ld5MQJEA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a word to index(word2idx) for source and target\n",
        "source_word2idx= dict([(word, i+1) for i,word in enumerate(source_words)])\n",
        "target_word2idx=dict([(word, i+1) for i, word in enumerate(target_words)])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCVdg9AzJHHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating a dictionary for index to word for source and target vocabulary\n",
        "source_idx2word= dict([(i, word) for word, i in  source_word2idx.items()])\n",
        "# print(source_idx2word)\n",
        "target_idx2word =dict([(i, word) for word, i in target_word2idx.items()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c1lBWa0JJKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Shuffle the data\n",
        "lines = shuffle(lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZtvDvAnJLuC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e1a23f44-7996-4c79-c6e9-3ba8dd3926ae"
      },
      "source": [
        "# Train - Test Split\n",
        "X, y = lines.source, lines.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((18507,), (2057,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G0g3CjEJOpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input tokens for encoder\n",
        "num_encoder_tokens=len(source_words)\n",
        "\n",
        "# Input tokens for decoder zero padded\n",
        "num_decoder_tokens=len(target_words) +1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x8a5dpYJRRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
        "    ''' Generate a batch of data '''\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            encoder_input_data = np.zeros((batch_size, max_source_length),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, max_target_length),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, max_target_length, num_decoder_tokens),dtype='float32')\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                for t, word in enumerate(input_text.split()):encoder_input_data[i, t] = source_word2idx[word] \n",
        "                for t, word in enumerate(target_text.split()):\n",
        "                    if t<len(target_text.split())-1:\n",
        "                        decoder_input_data[i, t] = target_word2idx[word] # decoder input seq\n",
        "                    if t>0:\n",
        "                        # decoder target sequence (one hot encoded)\n",
        "                        # does not include the START_ token\n",
        "                        # Offset by one timestep\n",
        "                        #print(word)\n",
        "                        decoder_target_data[i, t - 1, target_word2idx[word]] = 1.\n",
        "                    \n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvZ9leMEJUG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_samples = len(X_train)\n",
        "val_samples = len(X_test)\n",
        "batch_size = 128\n",
        "epochs = 50\n",
        "latent_dim=256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXKP7Rc4JWvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkiduCcDJZSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ49igAWJbKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the model that takes encoder and decoder input \n",
        "# to output decoder_outputs\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1qZUOQoJd0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkHKP5PwJfzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_samples = len(X_train) # Total Training samples\n",
        "val_samples = len(X_test)    # Total validation or test samples\n",
        "batch_size = 128\n",
        "epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4fhrRExJkqZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "bfeb083a-3deb-47c4-f687-96074419d049"
      },
      "source": [
        "model.fit_generator(generator = generate_batch(X_train[:1000], y_train, batch_size = batch_size),\n",
        "                    steps_per_epoch = train_samples//batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
        "                    validation_steps = val_samples//batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 144 steps, validate for 16 steps\n",
            "Epoch 1/10\n",
            "  3/144 [..............................] - ETA: 2:45 - loss: 1.3727 - acc: 0.0587"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoLNqQepJnPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}